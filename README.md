# 机器学习规则：关于机器学习工程的最佳实践
马丁·辛克维奇

这个文档旨在帮助有机器学习基础知识的人员从google的机器学习最佳实践中受益。它呈现了机器学习范式，类似google的c++规范和其他常用的实用编程最佳实践。如果你上过机器学习的课程或者已经开发过模型，那么你已经具备了读懂这篇文档的基础知识背景。

## 术语
以下名词会被经常提及。

**实例**：你要预测的事情，比如，一个实例可能是你想分成"和猫有关"和"和猫无关"的页面

**label**：标签，你要预测任务的结果-这个结果不是产生于机器学习系统就是由训练集提供。比如，标签结果就是"和猫有关"

**Feature**：特征，预测实例的一个基础属性，比如一个页面可能有个变量叫：包含了单词cat

**Feature Column**：特征集，一堆相关的变量集合，比如用户生活的潜在国家集合。变量集合在vm系统中也叫命名空间。

**example**: 样例，包含了变量集和标签结果的实例

**model**：模型，一个预测任务的统计表述，你用样本集训练了一个模型，然后用这个模型进行预测

**Metric**： 指标，你关心的一些东西，和直接的优化目标相关也可能不相关

**Objective**: 目标，你的算法尝试优化的一个指标

**Pipeline**: 工作流，围绕机器学习算法的基础构件，包括从源头采集数据，放入训练数据文件，训练一次或多次模型，以及把模型部署到生产

## 概述
To make great products:
do machine learning like the great engineer you are, not like the great machine learning expert you aren’t.'''
把自己当成一位出色的工程师，而不是一位机器学习专家。

实际上，你面临的问题大部分是工程问题（engineering problems）。即使是拥有所有资源的顶尖机器学习专家，大部分的提升来源于好的变量，而非算法本身。所以，进行机器学习的基本的方法是：
1. 确保管道从头到尾是稳固的
2. 从指定合理的目标开始
3. 用简单的方法添加常识特征
4. 确保管道始终稳固可靠
上述方法将长时间让你取得很好的效果。只要你仍旧可以通过简单的技巧取得进展，就不应该脱离上述方法。增加复杂会减缓未来的发布速度。
当你真的黔驴技穷的时候，领先的机器学习可能是尝试的方向。
以下文档主要分成四部分：
1. 第一部分帮助你了解构建机器学习系统的时机是否成熟
2. 第二部分是介绍部署第一个机器学习设施
3. 第三部分有关当你新增变量的实时，如何在机器学习设施中发布和迭代模型，怎样评估模型以及调整训练偏差
4. 最后一部分讲当进入模型瓶颈期的时候可以怎么做
5. 之后，提供了参考资料

（本节主要阐述了文档的整体思路，还是很赞同他说的： 机器学习问题面对的主要是工程问题，大部分效果的提升来源于好的变量，而非算法本身。但是，我觉得漏了一个前提，即每个人都对机器学习过程的理解程度决定了算法的上限。如果每个人都是同样的数据，了解同样的算法，那么天花板是相同的，但是如果对比一个懂算法和不懂算法的人，两者的差距是比较明显的，所以解读这句话需要读懂其背后隐藏的逻辑。）

## 在实施机器学习之前
###  法则1: 不要害怕发布未采用机器学习技术的产品
机器学习很酷，但是它需要数据。理论上，你能从另外一个问题中获取数据，并做微调，复用在一个新产品上，但其效果很可能不如基本的启发式算法。如果你觉得机器学习会给你带来100%的提升，可能启发式算法可以带来50%的提升。
比如，你想给apps进行排名，你可以用安装率或者安装量。如果你在监测垃圾信息，把之前发布过垃圾信息的发布者过滤掉。不要害怕用人工规则。如果你需要给联系人排序，用最近使用频次最高或者用字母顺序。如果机器学习在你的产品中不是必须的，则在有数据之前不要用它。

### 法则2:首先，设计和实现评价指标
在构建具体的机器学习系统之前，对现有系统记录尽量详细的信息。原因如下：
1. 更早获取用户的授权
2. 假设未来某些指标需要考虑在内，最好立刻开始收集相关的历史数据
3. 你脑中带着指标度量来设计系统，对你来说，未来会更好。具体的说，你不希望发现你自己正在从日志中处理字符串来统计指标
4. 你会注意到什么是变化的，什么是不变的。例如，假设你直接优化日活用户数。但是，在你早期的系统优化中，你可能发现用户体验的显著提升不会该指标发生明显变化。

google+团队关注平均阅读时长和平均分享次数，评论，每用户点评次数，每用户分享次数等等，用来计算帖子的质量。同时，注意到实验框架非常重要的，一种用于用户分捅并分组统计的实验框架。详见Rule#12

通过收集更多的指标，你可以更加全面了解你的系统。发现问题了？加指标去监控。激动的看到上次发布有定量的变化？加指标去监控。

### 法则3:选择机器学习技术而非复杂的启发式算法
简单的启发式算法有利于推出产品，但复杂的启发式算法难以维护。当你获取足够的数据并确定自己要尝试实现的目标后，考虑使用机器学习技术。与大多数软件工程任务一样，你需要不断更新方法，而且你会发现机器学习模型更易于更新和维护。（请参阅规则16）

## 机器学习阶段一：你的第一个管道
第一个管道多关注你的系统设施。很难想象如果你不信任你的管道会发生什么。
### 法则4: 第一个模型要简单，基础架构是正确的
第一个模型会给你的产品带来最大的提升，所以并不需要很花哨。但是你会碰到比你想象中更多的架构问题。在其他人使用你的创意十足的机器学习系统之前，你需要：
1. 机器学习系统如何获取样本
2. 系统如何区分好和坏
3. 如何把模型整合到你的应用中。你要么让模型在线服务 或者离线训练好模型然后存成结果表。例如，你会离线对网页进行分类并存储在结果表，但是你会对聊天消息进行实时分类。

** 选择简单的特征，你更容易确保：**
1. 算法的变量是正确的
2. 模型学习到了合理的权重
3. 模型服务中使用了正确的变量
一旦你的系统可靠的完成这3件事情，你的大部分工作都做完了。第一个简单的模型给你提供了基线，用于和更复杂的模型进行对比。有些团队的目标是发布一个中立的模型:明确区分机器学习收益的优先级，以避免分心

### 法则5: 独立于机器学习来测试系统流程
确保基础架构是可测试的。系统中的算法部分独立封装，以便测试算法以外的其他流程。具体而言：
1. 测试给算法的数据准确性。检查应填充的特征列是否已填充。在隐私许可的情况下，手动检查输入到算法的数据。如果可能的话，查看管道中的数据统计结果是否与其他地方处理的结果相同
2. 测试从训练算法得出的模型效果，确保训练环境中的模型和生产环境的的模型结构相同（参考第37条规则）
机器学习具有不可预测性。确保你在训练和生产上都做过了相同样本的测试来保证代码的正确性，你可以在线上使用同一个固定的模型。当然，理解你的数据是重要的。

### 法则6: 复制管道时留意丢掉的数据
我们常复制现有的管道来生成新的管道，新管道需要的数据可能在旧管道是被丢弃的。一种常见模式是仅记录用户看到的数据。因此，如果我们想要对用户看不到特定帖子的原因进行建模，此类数据就毫无用处，因为管道已丢弃所有负样本。
因此代码从一个场景复制到另一个场景的时候，需要留意一下两个场景对于数据的要求是否一致。
### 法则7: 把启发式规则变成特征，或者在外部处理它们
通常，机器学习要解决的问题不一定是全新的。有现成的系统，排序，分类或者其他你想解决的问题，这意味着存在许多规则和启发式规则。这些启发式规则和机器学习结合时会带来提升。启发式规则的信息应该被充分挖掘，基于两个理由。第一个是过渡到机器学习系统会更顺畅。第二个是规则含了很多直觉信息是你不愿放弃的。你有四种方法来使用现存的启发式规则：
1. **用启发规则进行预处理**。如果规则非常有用，则直接执行即可。例如在垃圾邮件识别中，如果发件人已经被拉黑了，那么就不要再去学“拉黑”意味着什么，直接拉黑就好了。这种方法最适合在二元分类任务中使用。
2. 变成特征。直接从启发式规则里面创建特征是很好的做法
3. 挖掘启发式规则的原始输入。如果某个应用启发式算法结合了安装次数、文本中的字符数以及星期值，考虑将这些内容拆分开来，并作为输入单独提供给学习算法。部分适用于集成学习的技巧也适用于此（请参阅第 40 条规则）
4. 修改标签。当你觉得启发式规则会获取当前标签未包含的信息时，可以选择此操作。例如，你正在最大化下载次数，但是同时也关注内容质量，这可能方案是标签乘以应用的平均评分。你可以灵活的修改标签。
已有的启发式规则可以帮助机器学习更平稳的过渡，但也需要考虑是否有更简单的方式，有没有增加系统的复杂度。

### 监控
一般来说，请实现良好的监控机制，例如设计解决报警的步骤以及提供可视化监控报表
### 法则8: 了解系统对新鲜度的要求
如你的模型是一天前的，效果会下降多少。一周呢？一个季度呢？这些信息将告诉你对于监控的优先级。如果你的模型一天没更新会损失10%的收入，那么有一个工程师持续观察是合理的。大部分的广告系统每天都会上线新的广告，因此必须每日更新。更新频次因不同的应用和场景而定。此外，新鲜度也会因时间而异，特别是你的模型新增或者移除特征时。
### 法则9: 先检测问题，再导出模型
大部分的机器学习系统都会经过导出模型到应用的阶段。如果模型导出有问题，就是面向用户的问题了。模型上线前一定要做完整性、正确性检查，例如AUC,NE,RMSE等指标。如果是模型上线前出了问题，可以邮件通知。如果是面向用户的线上模型出了问题，就需要电话通知了。在影响用户之前，确保模型没问题。
### 法则10： 注意隐藏的数据问题
相比较其他系统，机器学习系统更容易出现这类问题。例如你关联的表数据不再更新了，机器学习模型会进行调整，效果会不断的下降，但是短期内看不出来。有时候会发现数据已经几个月没有更新了，只需刷新一下，就会比季度内做的其他改进提升的效果更多。或者特征的定义有变化，变量的覆盖度从90%下降到60%。如果你对数据统计进行监控，加上定期人工抽查，可以减少此类问题。
### 法则11：给特征组分配负责人，并写下文档
如果系统庞大，有很多特征列，那么知道每组数据由谁生成或者在维护就变得非常重要。虽然数据都有简单描述，但是关于特征的具体计算逻辑，数据来源等有详细的记录就是一个很好的做法。
### 你的目标
系统有很多指标或者度量，但是机器学习系统只能指定一个目标，即算法尝试优化的目标。我区别一下目标和指标，目标（objective）是模型试图优化的值，而指标（metric）指的是任何用来衡量系统的值。

### 法则12: 不要过于纠结该优化哪个目标
现实中，你会关心很多指标。你在机器学习的早期阶段，你会注意到所有的指标都在上升，即使你不是直接优化这些指标。不要混淆你的目标和系统最终的健壮性。当你发现优化的指标在上升，但是你决定不发布时，你的优化目标是需要调整的。

### 法则13: 你的第一个目标选择简单，可观察可归因的简单指标**
你常不知道真正的目标是什么。机器学习的目标应该是容易测量的或者是目标的替代结果。选择简单的目标进行训练，考虑最上面有个代理层，加点额外的逻辑在上面完成最终的排序。
建模最简单的目标是，直接观察或者归因的用户行为：
1. 链接是否被点击
2. 目标是否被下载
3. 目标是否被转发、回复或者回复
4. 目标是否被点评
5. 展示的结果是否标记为垃圾邮件。

避免对间接的结果进行建模：
1. 用户隔天是否会回访
2. 用户多长时间来访问网站
3. 日活用户是哪些

间接的行为是很好的评价指标，可以用于ABtest，决定系统是否发布。
最后，不要试着让机器学习琢磨：
1. 用户使用产品是否开心
2. 用户体验是否满意
3. 产品是否改进了用户的幸福感
4. 这会如何影响公司的整体健康度
这些都很重要，但是非常难评估。相反，用替代方法。如果这个用户是开心的，他们在网站上待的时间更长。如果用户是满意的，他明天还会再来。就福利和公司健康度而言，需要人的判断，以便将机器学习目标与你所销售产品的性质和商业计划联系起来。

### 法则14: 从一个可解释的模型开始，方便调试
线性回归、逻辑回归和泊松回归均由概率模型直接驱动。每个预测结果都可以看做是一个概率或预期值，这比用直接优化目标分类准确性或者排序效果（0-1 损失，hinge 损失函数等）等模型容易调试多了。例如，如果训练中得出的概率和生产系统中或者并行系统中预测的概率有偏差，则表示存在问题。
例如，在线性回归、逻辑归回和泊松回归中，部分数据集的平均预测期望值等于平均标签结果。假设你没有正则化且算法已收敛，那么理论上是这样的，实际上也差不多是这种情形。
借助于简单的模型，你更容易形成反馈闭环（参考第36条规则）。通常，我们会依据这些概率预测来做决策。例如，根据期望值（点击/下载概率等）为标准对帖子进行降序排名。不过，请留意，当选择模型时，你的决定比模型给出的概率更为重要（参考27条规则）
### 第15条规则： 在策略层把垃圾内容过滤模块和质量排序模块进行解耦
质量排序是门艺术，但是垃圾内容过滤是场战争。对于用户来说，确定高质量帖子的因素更显而易见，而且用户会调整自己的帖子，使其具有高质量的属性。你不应该因为质量排序学习器将垃圾内容排在前面而对其应用打折扣。同样，『少儿不宜』的内容也不应该在质量排序中进行处理。垃圾内容过滤 则另当别论。你必须明白，生成的特征会不断变化。通常情况下，你在会在系统中设置一些明显的规则（如果一个帖子收到三次以上的垃圾内容举报，请勿显示此帖子等等）。机器学习模型必须每天更新。内容创作者的声誉会发挥很大作用。
在某个地方，必须将两个系统的结果整合在一起。请留意，与过滤电子邮件中的垃圾邮件相比，搜索结果中垃圾内容过滤更加主动。此外，从质量分类器的训练数据中移除垃圾内容也是一种标准做法。

# 机器学习第二阶段： 特征工程
在机器学习系统生命周期的第一阶段，重要的问题涉及以下三个方面： 将训练数据导入学习系统、对目标指标进行测量，构建应用基础架构。当你构建了一个端到端的稳定运行的系统，并且制定了系统测试和单元测试后，就可以进入第二阶段了。
第二阶段，有很多唾手可得的成果，且有很多明显的特征可导入系统。因此，第二阶段涉及导入尽可能多的特征，并以直观的方式将他们组合起来。在这个阶段，所有的指标应该是上升的。将会有很多次的发布，是时候安排多名工程师，以便整合所有数据来创建真正出色的学习系统。
### 法则16: 制定发布和迭代模型计划
不要指望你现在构建的模型是你最后一个发布的模型，也不要指望你会停止发布模型。考虑一下你这次做的变动的复杂度是否会拖慢未来的更新速度。许多模型按照季度甚至年度进行更新。发布新模型有三种原因：
1. 你新增了变量
2. 你在旧模型上有调优或者用了新的方法
3. 优化了你的目标函数
构建模型时，考虑新增、删除或者重组变量是否容易，考虑复制全新的管道并验证争取正确性是否容易，考虑是否并行跑2到3个副本难易程度。最后，不要担心这个版本是否包含了所有的特征，下个季度在新增。

### 法则17: 从可观察和报告的特征（而不是已经学习的特征）着手
这一点可能存在争议，但可以避免许多误区。首先介绍一下什么是学习的特征。学习的特征是由外部系统（例如非监督系统）或者学习器本身（例如通过因子分析或者深度学习）生成的特征。这两种方式生成的特征都非常有用，但会导致很多问题，因此不应该在开始的模型中使用。
如果你用外部系统创建了一个变量，要知道它有自己的目标。外部系统的目标可能和当前的目标相关性比较弱。如果你获取外部系统的某个瞬间状态，它可能已过期。如果你从外部系统更新特征，特征含义可能会发生变化。如果你使用外部系统特征，你需要非常小心。
因子模型和深度模型的主要问题是，它们是非凸模型。因此无法保证能够模拟或者找到最优解决方案，且每次迭代时找到的局部最小值可能不同。这种变化导致难以判断系统发生的某次变化的影响是有意义的还是随机的。通过没有使用深度特征的模型，你可以获得一个很好的基线模型。在这个基线之后，你可以尝试更深奥的方法。

### 法则18: 探索可跨情境泛化的内容的特征
机器学习系统通常只是更大系统中的一小部分。例如，你想象有个帖子被指定成热门帖子，在它成为热门帖子之前，很多用户已经对其收藏、转发、评论。如果你把这些统计指标放在学习器中，它就会对正在优化的场景中没有数据的新帖子进行推广。youtube的『接下来观看』可以使用youtube搜索的观看次数或连看次数（观看完一个视频后观看另一个视频的次数）或明确的用户评分类推荐内容。最后，如果你将用户行为作为标签，在其他情境中看到用户对文档执行该操作可以是很好的特征。借助于这些特征，你可以在该场景中引入新内容。请注意，这与个性化无关：线弄清楚是否有人喜欢此场景中的内容，然后再弄清楚喜欢程度。

### 法则19: 尽可能使用非常具体的特征
对于海量数据，学习数百万个简单的特征比学习几个复杂的特征更简单。被检索的文本的标识符和规范化的查询不会提供泛化作用，只会调整头部查询中的标记查询。因此，请不要害怕使用具有以下的特征组：每个特征只应用于一小部分的数据，但是总体的覆盖率超过90%。你可以用正则化来消除哪些覆盖样本太少的特征。

### 法则20：组合和修改现有特征，以简单易懂的方式创建新特征
组合和修改特征的方式有很多。机器学习系统，例如tensorflow，允许你直接通过『transformations』来做数据预处理。标准的两种处理方法是：离散化和组合。

离散化是把连续特征，创建许多离散特征。以年龄为例，你可以把小于18岁赋值成1的特征，并创建年龄在18-35周岁之间时其值为1的另一个特征。不要过多考虑直方图的边界：分位数会给你带来很大的影响。

组合方法会组合两个或者更多个特征列。在tensorflow中，特征列指的是同类特征集，例如{男性、女性}，{美国、加拿大、墨西哥}等等。组合指的是其中包含特征的新特征列，例如，{男性，女性}*{美国，加拿大，墨西哥}。此特征列包含特征{男性，加拿大}。如果你使用tf，并让tf为你创建此组合，则此{男性，加拿大}将存在表示加拿大男性的样本中。请留意，如果你拥有大量的数据，才去使用三个、四个或者更多更多特征组合的机器学习模型。

生成非常大的特征组合可能会过拟合。搜索中，如果你的文档和query里面都包含了词，你可以进行组合，这个组合量非常大。

### 法则21：线性模型中的特征权重数量大致和数据量成正比
评估模型复杂度有很多花哨的统计学理论，你基本上只要了解这条规则即可。有人认为从一千个样本中并不能得到什么可靠的训练结果，或者由于选择了某些特定的模型，必须要一百万的样例，否则就没法展开模型训练。关键是根据你的样本量来调整模型方法。
1. 如果你负责搜索排序系统，文档有上百万的词语，查询句子有1000个标记的样本，你可以使用文档和查询词条的特征进行点乘，特征有IF-IDF和其他人工造的特征。
2. 如果你有上百万的样本量，使用正则化和特征选择组合后的文档特征和查询特征列。这将造出上百万的特征，使用正则化可以降低特征数量。上百万的样本集，百级别的特征。
3. 如果你有数十亿或数千亿个样本，你可以使用特征选择和正则化，通过文档和查询标记组合特征列。你会有十亿个样本，一千万个特征。统计学习理论很少设定严格的限制，但能够提供很好的起点。
最后，请根据第28条规则决定要使用哪些特征。

### 法则22: 清理掉不再使用的特征
不使用的特征会留下技术债。如果你发现有些特征没有用了，那么其余其他特征组合在一起就不起作用，则要将这个特征从基础架构中删除。你需要让自己的基础架构保持简洁，以便尽可能快的尝试能够来效果提升的特征。如有必要，他人可以随时将你的特征加回来。
在决定要添加或者保留特征时，要考虑到覆盖率，即相应特征覆盖了多少个样本？录入，如果你有一些个性化的特征，但只有8%的覆盖率，那效果不会很好。
与此同时，一些特征覆盖率低的特征也可能很重要。例如，如果你有一个只覆盖1%的数据量，但是覆盖这个特征的，90%样本的特征是正面的，那么这将成为一个很重要的特征。

# 对系统的人工分析
在继续探讨机器学习的第三阶段之前，请务必重点了解一下在任何机器课程都无法学到的内容：如何检查现有模型并加以改善。这更像是一门艺术而非科学，但是有几个需要避免的反模式。
### 法则23:你不是一个典型的终端用户
这可能是一个团队陷入困境的最简单的方法。你对代码太熟悉了。你关注的可能只是帖子的某个特定方面，或者你只是投入了太多感情。其次，你的时间很宝贵。
如果你确实想获得用户反馈，使用用户体验方法。在流程的早期阶段开展用户画像和可用性测试。例如，如果你的团队成员都是男性，这有必要设计一个35岁的女性用户角色，并查看生成的结果，而不是只查看10位25-40岁的男性的用户画像结果。在可用性测试中请用户体验你的网站（通过本地或远程方式）并观察他们的反应也可以给你提供全新的视角。

### 法则24：衡量模型迭代结果**
在发布最新的模型之前，你可以进行一个最简单（也是最有用）的一项衡量，评估一下新模型和线上模型有多大差别。例如，搜索排序系统，这在系统中对同一批查询请求运行这两个模型，查看两个结果的差距有多大。如果差距非常小，就无需运行试验了，可以判断不催出现很大变化。如果差距很大，那么需要确保这中更改可以带来好的结果。查看结果影响比较大的查询请求有助于你这个变动的可靠性。不过，前提是确保系统是稳定的，模型自身的变动较小。

### 法则25: 选择模型时，实际效果比预测能力更重要
你可能会尝试预测点击率。但归根到底，关键问题是你用这个预测做什么。如果你用来做排序，最终排序的质量比预测本身更重要。如果你要预测垃圾文档的概率，那么选择一个阈值来确定要阻止的内容，那么通过的内容准确性更为重要。大多数情况下，这两项是一致的。当它们不一致时，差距也应该不大。因此，如果某种更改可以显著提升logloss，但是会降低系统的实际效果，则放弃这个改变。当这种事情频繁发生时，说明你该重新审视模型的目标了。

### 法则26: 在预测错误中寻找规律，并创建新特征
假如你发现模型在某一个样本预测错了。在分类任务中，这可能是误报或漏报，在排序系统中，可能是一个正向判断弱于逆向判断的结果。但更重要的是，让机器学习系统知道它错了，需要修正。如果你此事给模型一个修复的特征，那么模型将会尝试使用这个特征来修复这个问题。
另外，如果你基于正确的样例来创建特征，那么该特征可能被系统忽略。例如，谷歌play商店的应用搜索中，有人搜索『免费游戏』，但其中一个排序靠前的搜索结果却是一款其他APP,所以你为其他APP创建了一个特征。但如果你将其他APP的安装数最大化，即人们在搜索免费游这个戏时安装了其他APP，那么这个其他app的特征就不会产生其应有的效果。
所以正确的做法是，一旦你发现了模型预测错误的样本，应该在当前的特征集之外寻找解决方案。例如，如果你的系统降低了内容较长的帖子的排名，那就应该增加有关帖子长度的特征，不要拘泥于具体的细节。加入你要增加帖子长度的特征，就不要猜测长度的具体含义，而应该增加几个相关的特征，交给模型自行处理，这才是最简单有效的方法。

### 法则27:尝试量化观察到的异常行为
团队成员会对现存的损失函数捕捉不到的系统属性感到无能为力，但是这个抱怨没用的。如果问题可以被量化，你可以作为特征、目标或者度量。总之，先量化，再优化。

### 法则28：注意短期行为和长期行为的差异
如果你有个系统，用于查询文档和查询请求，然后根据每个文档的每次查询行为计算点击率。你发现它的行为几乎与当前系统的并行和A/B测试结果完全相同，而且它很简单，然后你上线了这个系统。却没有新的应用显示，为什么？ 由于你的系统只基于自己的历史查询记录显示文档，所以没法显示新的文档。
要了解一个系统在长期行为中如何工作的唯一办法，就是让它只基于当前的模型数据展开训练。

# 离线训练和实际线上服务的偏差

** 出现偏差的原因如下：**
1. 训练和部署的处理数据的方式不一样
2. 训练和部署使用的数据不同
3. 算法和模型之间的一个循环反馈（有点抽象？）

### 法则29：确保训练和在线服务最好的方式是保存在线服务的特征值，然后在后续训练中直接使用这个特征
即使你不能保存全部的样本，保留部分也是有用的，这样你就能确保训练和在线服务的一致性。（见法则37）。google采取了这种方法的团队有时候会对其效果感到惊讶。比如youtube主页在服务时会切换到日志记录特征，这不仅大大提高了服务效果，而且减少了代码复杂度。目前很多团队都已经在其基础设施上采用了这种策略。

### 法则 30:给采样数据按重要性大小加权，不要随意丢弃它们
当你有太多数据的时候，总忍不住想要丢弃一些，以减轻负担，这绝对是个错误。有好几个团队因为这样，而引起了不少问题（法则6）.尽管那些从来没有展示给用户的数据的确可以丢弃掉。对于其他的数据，最好还是对重要性大小加权。比如如果你决定抽样30%的样例数据，则赋予权重10/3。使用重要性大小加权并不影响法则14中讨论的校准属性。
### 法则31: 注意在训练和服务时都会使用的表的数据可能是变化的
因为表中的特征可能会改变，在训练和服务时的值不一样，这会造成，哪怕是相同的文档，你的预测结果和训练结果是不同的。避免同类问题最简单的方式是在服务时将特征取对数（参阅法则32）。如果表格数据变化比较缓慢，你可以通过按小时或者每天给表创建快照的方式来保证尽可能接近的数据。但这不能完全解决这个问题。

### 法则32： 尽可能的在训练和在线服务间重用代码
批处理不同于在线处理。在线处理中，你必须及时处理每一个请求（比如，必须为每个查询单独查找），而批处理，你可以合并任务（例如，做join）。服务期间，你是在线处理，但是训练是批处理任务。尽管如此，还是有一些方法重用代码。例如，你可以专门为自己的系统创建一个对象，其中所有的查询结果和join都以易读的方式进行存储，且容易把错误测试出来。当你收集了所有信息后，你可以在服务和训练间使用同一种方法，使用特定的对象来确定机器学习系统的预期结果。这将会消除训练和服务偏差的一个来源。由此，训练和服务尽量不要使用两种不同的编程语言。如果这样做，就不会无法共享代码了。

### 法则33： 如果你根据1/5号之前的数据生成模型，则对1/6日之后的数据进行模型测试
通常来说，要衡量模型的效果，应使用在训练模型所有数据对应日期之后收集的数据，因为这样才能更好及时反馈应用到生产。这条法则的核心思想是要留意一下有时间线的模型，不要把未来发生的数据放在了现在的预测特征中。假如你在1/5号之前训练了一个模型，从1/6号开始测试模型效果。你会发现模型在新数据会差一些，但不可能显著变差。虽然有可能存在一些日常影响，你可能没有预测到平均点击率或转化率，但AUC（表示正分类样本的分数高于负分类样本的概率）应该非常接近。

### 法则34: 在二分类中，为了获得干净的数据可以牺牲短期的效果
在过滤的场景，负样本是不会展示给用户的。假如线上系统过滤掉75%的负样本，那么你可能需要从向用户显示的实例中提取额外的训练数据并展开训练。比如说，用户将系统认可的邮件标记成垃圾邮件，那么你可以需要从中学习。
但这种方法会引入采样偏差，如果改在服务期间将所有流量的1%标记成『暂停』，并将这样的样例发送给用户，那你就能收集更纯净的数据。现在你的过滤器阻止了只是奥74%的负样本，这些样例可以成为训练数据。

需要注意的是，如果你的过滤器阻止了95%或者更多的负样本，那这种方法可能就不太适用。不过即使如此，如果你想衡量服务的性能，可以选择更细致的采样（例如0.1%或0.001%），一万个样例足以准确性评估性能。

### 法则35： 注意排序问题的固有偏差
当你改变排序算法时，一方面会引起排序结果完全不同，另一方面也可能很大程度上改变了算法未来要处理的数据，这会引入一些固有偏差。以下方法可以有效帮你优化训练数据：
1. 对涵盖更多查询的特征进行更大的正则化，而不是那些只覆盖单一查询的特征，这种方式使得模型偏好针对个别查询的特征，而不是那些能够泛化到全部查询的特征。这种方法能够阻止非常流行的结果进入无关查询结果。这点和传统的建议不一样，传统建议应该对更独特的特征集进行更高的正则化。
2. 只允许特征有正向权重，这样一来就能保证任何好特征都会比未知特征更好。
3. 不要用那些只有文档的特征。比如，不管搜索请求是什么，即使给定的应用程序是当前的热门下载，你也不会想在所有地方显示它。

### 法则36： 避免具有位置特征的反馈回路
内容的位置会显著影响用户和它交互的可能性。显然，如果你把一个APP置顶，点击量一定上升。处理这类问题的一个有效方法是加入位置特征，即关于页面的内容的位置特征。如果你用位置特征训练模型，那模型就会更偏向"1st位置"这类特征，因而对于那些1st=true的样例的其他因子被赋予更低的权重。而在服务的时候，你不会给出任何位置特征，或者你给他们所有相同的默认特征。因为你在决定按什么顺序展示前，你已经限定了候选集。
切记，将任何位置特征和模型的其他特征保持一定的分离是非常重要的。因为位置特征在训练和测试时不一样。理想的模型是位置特征函数和其他特征的函数的和。不要将位置特征和文本特征交叉。

### 法则 37： 测量训练/在线服务偏差
很多场景会引起偏差，大致有如下几种：
1. 训练和验证数据之间的性能偏差，一般来说，总是存在的，但并不总是坏事。
2. 验证集和新时间生成数据之间的性能偏差，也总是存在的。你应该调整正则化来最大化的新时间生成数据上的效果。但是，如果这种性能差异很大，那可能说明采用了一些时间敏感的特征导致模型效果下降明显；
3. 新时间周期数和线上数据上的性能有差异。如果你使用相同的样本数据分别喂给离线模型和服务结果，它们应该给出相同的结果。因此，出现这个差异可能意味着出现了工程问题。

# 机器学习第三阶段: 缓慢增长、优化细化和复杂模型
第二阶段即将结束时会出现一些信号。首先，月增长开始减弱，你开始要考虑在一些指标间权衡。在某些测试中，一些指标增长了，而有些却下降了。这将会变得越来越有趣。增长越来越难实现，必须要考虑更加复杂的机器学习算法。
注意： 相比之前部分，这部分会有更多的纯理论性原则。第一阶段和第二阶段的机器学习总是很快乐的。当到了第三阶段，他们西部找到自己的道路。

### 法则 38： 如果业务目标和模型目标不一致，并成为问题，就不要在新特征上浪费时间
当达到性能瓶颈时，你的团队开始关注机器学习系统目标范围之外的问题。如同之前提到的，如果业务目标和算法目标不同时，你就得修改其中一个。比如说，你也许优化的是点击数、点赞或者下载，但发布决策还是依赖于人工评分者。
### 法则39：发布决策代表的是长期产品目标
alice有一个降低安装预测逻辑损失结果的想法。他增加了一个特征，逻辑loss下降了。当线上测试的时候，他看到实际的安装率提升了。当他召集发布复盘会时，有人指出日活用户数下降了5%，于是团队决定不发布该模型。alice很失望，但意识到发布决策依赖于多个指标，而仅仅只有一些是机器学习能够直接优化的。

真实的世界不是网络游戏：这里没有『攻击值』和『血量』来衡量产品的健康状况。团队只能靠收集统计数据来有效的预测系统在将来会如何。他们必须关心用户粘性，日活，30天日活，收入及广告主的收益。这些A/B测试中的指标，实际上只是长期目标的替代：让用户满意，增加用户，让合作方满意还有利润；甚至你会考虑有用高质量的产品和五年之外的公司繁荣。

当所有的指标都很好（或者不比原来差的）的时候，很容易下发布的决定。如果要在复杂的机器学习算法和简单的专家系统中选择，如果专家系统在所有的指标中更胜一筹，那应该选择专家系统。另外，所有指标数据并没有明确的孰轻孰重。每一个指标覆盖了一些团队关注的风险，但没有指标能够覆盖团队的首要关切：我的产品在五年后会怎样。

另一方面，个体更倾向于那些他们能够直接优化的单一目标。大多数机器学习工具也如此。在这个前提下，一个能创造新特征的工程师总能稳定的发布产品。多目标机器学习开始处理这类问题，比如给每个目标设定最低接线，然后优化指标的线性组合。但即便如此，也不是所有指标都能轻易表达成机器学习目标:如果一篇文章被点击或者一个app被安装了，这可能只是因为内容被展示了。但要搞清楚为什么一个用户会访问你的网站就很难了。如果完整的预测一个网站将来是否能成功是一个AI完全问题，就和CV和NLP一样难。

### 法则40： 保持集成模型的简洁性
使用原始特征，直接对内容排序的单一模型，是最容易理解和debug的。但是，一个集成模型（一个把其他模型得分组合在一起的『模型』）效果会更好。为保持简洁，每个模型应该要么是一个只接受其他模型的输入的集成模型，要么是一个有多个特征的基础模型，但不能两者皆是。如果在单独训练的模型之后还融合了其他模型，结合之后可能会导致效果不好。
使用简单的模型进行集成学习（仅将基本模型的输出作为输入）。此外，你还需要将属性强加到这些集成模型上。例如，基础模型的分数的提升不应会集成学习模型的效果有所降低。另外，如果传入的模型在语义上可解释，则最理想。因为这样一来，即使基本模型发生改变，也不会扰乱集成学习模型。另外，强制要求：如果基本分类器的预测概率变大，不会使集成学习模型的预测概率降低。

### 法则41： 效果平稳后，寻找与现有信息有质的差别的来源添加进来，而不是优化现有的信息
你添加了一些用户的人口统计信息，也添加了一些文档中词语信息。你探索了模板，并调整了正则化。但在几个季度的发布中，关键指标的提升幅度从来没有超过1%。现在你该怎么办?
是时候开始为截然不同的特征（例如，用户过去一天内，一周内或一年内访问的文档历史记录，或者其他属性的数据）构建基础架构了。你可以使用wiki条目或公司内部信息（例如，google的知识图谱）。利用深度学习，调整你对投资回报的期望，并付出相应的努力。和其他工程项目一样，你必须为添加新特征和复杂度成本进行一番权衡。

### 法则42：不要期望多样性、个性化、相关性和受欢迎程度之间有紧密的联系
内容的多样性意味着很多东西，其中内容来源的多样性最为普遍。个性化意味着每个用户都能获得他感兴趣的结果。相关性意味着一个特定的查询对于某个查询总比其他更合适。显然，这三个属性的定义和标准都不相同。
问题是标准很难打破。

注意：如果你的系统在统计点击量、耗时、浏览数、点赞数等等。你事实上在衡量内容的受欢迎程度。有团队试图学习具备多样性的个性化模型。为了个性化，他们加入了个性化的特征（有些特征表示了用户的兴趣）或者多样性特征（表示该特征与其他返回文档有相同的特征，比如作者和内容），然后发现这些特征比他们预期得到了更低的权重（有时是相反的信号）。

这不意味着多样性、个性化和相关性不重要。就像之前的规则指出的，你可以通过后处理来增加多样性或者相关性。如果你看到长期目标增长了，那至少你可以声称除了受欢迎程度，多样性/相关性是有价值的。然后你可以继续后处理或者直接修改优化目标为多样性或者相关性。
### 法则43: 在不同的产品中，你的好友基本保持不变，但你的兴趣并非如此
谷歌团队经常把预测某产品联系紧密度的模型，应用在另一个产品上，发现效果很好。另一方面，我见过好几个产品在个性化上苦苦挣扎的团队。是的，之前看起来它应该能奏效。但现在看来不会了。有时候起作用的是，用某个属性的原始特征来预测另一个属性的行为。此外，请注意，仅仅是知道用户有其他属性的历史记录也会有帮助。例如，两个产品上出现了用户活动或许本身就可以说明问题。

# 相关资料
谷歌在内外部都有很多的资料
- [machine learning crash course](https://developers.google.com/machine-learning/crash-course/): 机器学习应用介绍
- Machine learning:A probabilistic Approach
- google paper on [technical debt](https://ai.google/research/pubs/pub43146),有很多的建议
